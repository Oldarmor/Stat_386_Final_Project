[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Python Package!",
    "section": "",
    "text": "Here is a website all about my package for STAT 386!\nDocumentation here.\nGet started here.\nReview the technical report."
  },
  {
    "objectID": "TechnicalReport.html",
    "href": "TechnicalReport.html",
    "title": "Technical Report",
    "section": "",
    "text": "TODO: Provide a concise overview of the project objectives, key findings, and actionable recommendations."
  },
  {
    "objectID": "TechnicalReport.html#executive-summary",
    "href": "TechnicalReport.html#executive-summary",
    "title": "Technical Report",
    "section": "",
    "text": "TODO: Provide a concise overview of the project objectives, key findings, and actionable recommendations."
  },
  {
    "objectID": "TechnicalReport.html#project-context",
    "href": "TechnicalReport.html#project-context",
    "title": "Technical Report",
    "section": "Project Context",
    "text": "Project Context\nTODO: Describe the motivation, stakeholders, and success criteria for this analysis."
  },
  {
    "objectID": "TechnicalReport.html#data-sources",
    "href": "TechnicalReport.html#data-sources",
    "title": "Technical Report",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary dataset: VG Sales Data\nSupplementary data: Scraped player data from Steam using aGithub Repo.Because of the lengthy time scraping the data took, we have a file of how we scraped it and the scraped csv in the package."
  },
  {
    "objectID": "TechnicalReport.html#methodology",
    "href": "TechnicalReport.html#methodology",
    "title": "Technical Report",
    "section": "Methodology",
    "text": "Methodology\n\nData acquisition: TODO - outline collection scripts/APIs\nCleaning pipeline: TODO - summarize transformations and validations implemented\nAnalysis workflow: TODO - describe statistical or modeling techniques\nTooling: TODO - list packages, environments, and reproducibility steps"
  },
  {
    "objectID": "TechnicalReport.html#results-diagnostics",
    "href": "TechnicalReport.html#results-diagnostics",
    "title": "Technical Report",
    "section": "Results & Diagnostics",
    "text": "Results & Diagnostics\n\nBest Hyperparameters:\nmax_depth: 10 min_samples_leaf: 1 min_samples_split: 5 n_estimators: 200\n\n\nPerformance Metrics:\nR² (Test Set): 0.6604 RMSE (log scale): 0.1269\n\n\nFeature Importance (Top 10):\nRank – 0.2906 Platform_X360 – 0.1819 last_30_day_avg – 0.1439 all_time_peak – 0.0575 Platform_PS3 – 0.0537 Platform_SAT – 0.0359 Platform_PS2 – 0.0356 Year – 0.0319 Platform_PC – 0.0297 Platform_GC – 0.0287\n\n\nInterpretation:\nRank is the most influential predictor, followed by platform-specific indicators and recent activity metrics (last_30_day_avg). Genre features did not appear in the top 10, suggesting platform and ranking dominate sales prediction."
  },
  {
    "objectID": "TechnicalReport.html#discussion-next-steps",
    "href": "TechnicalReport.html#discussion-next-steps",
    "title": "Technical Report",
    "section": "Discussion & Next Steps",
    "text": "Discussion & Next Steps\nTODO: Interpret the results, note limitations, and capture open questions or future experiments."
  },
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Project Functions Documentation",
    "section": "",
    "text": "This document describes the public functions in the project, covering their purpose, parameters, return values, side effects, usage examples, and implementation notes. The functions are defined across the following modules: read.py, preprocess.py, model.py, and viz.py."
  },
  {
    "objectID": "Documentation.html#get_data_pathfilename-str",
    "href": "Documentation.html#get_data_pathfilename-str",
    "title": "Project Functions Documentation",
    "section": "get_data_path(filename: str)",
    "text": "get_data_path(filename: str)\nPurpose: Return a path-like object pointing to data/&lt;filename&gt; within the package using importlib.resources.\nParameters: - filename (str): File name, e.g., \"sales.csv\".\nReturns: - Path-like object to data/&lt;filename&gt;.\nNotes: Ensure your package includes a data/ directory and resources are accessible.\nExample:\nfrom read import get_data_path\ncsv_path = get_data_path(\"sales.csv\")"
  },
  {
    "objectID": "Documentation.html#read_datafile_path",
    "href": "Documentation.html#read_datafile_path",
    "title": "Project Functions Documentation",
    "section": "read_data(file_path)",
    "text": "read_data(file_path)\nPurpose: Read CSV into a DataFrame; convert Year to pandas nullable integer (Int64); drop \"Unnamed: 0\" column; return cleaned DataFrame.\nParameters: - file_path: String or path-like to the CSV.\nReturns: - sales (pd.DataFrame): Cleaned dataset.\nSide effects: Changes Year dtype; drops \"Unnamed: 0\" if present.\nExample:\nfrom read import read_data, get_data_path\nsales = read_data(get_data_path(\"sales.csv\"))"
  },
  {
    "objectID": "Documentation.html#process_datasales",
    "href": "Documentation.html#process_datasales",
    "title": "Project Functions Documentation",
    "section": "process_data(sales)",
    "text": "process_data(sales)\nPurpose: Deduplicate and aggregate raw sales records to the Name level, collecting distinct categorical lists and summarizing numeric metrics.\nAggregations: - Platform: list of unique non-null platforms - Year: max - Genre: list of unique non-null genres - Publisher: list of unique non-null publishers - all_time_peak: max - last_30_day_avg: max - NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales: sum\nReturns: - sales_combined (pd.DataFrame) with one row per Name.\nExample:\nfrom preprocess import process_data\nsales_combined = process_data(sales)"
  },
  {
    "objectID": "Documentation.html#prepare_datasales_combined",
    "href": "Documentation.html#prepare_datasales_combined",
    "title": "Project Functions Documentation",
    "section": "prepare_data(sales_combined)",
    "text": "prepare_data(sales_combined)\nPurpose: Ensure list types for Platform/Genre; multi-hot encode them via MultiLabelBinarizer; concatenate encoded features with selected numeric predictors; drop rows with missing Year.\nReturns: - final_df (pd.DataFrame) containing: - Numeric predictors: all_time_peak, last_30_day_avg, Year, Rank - Target(s): includes Global_Sales - Encoded features: Platform_&lt;category&gt;, Genre_&lt;category&gt;\nNotes: Adjust which targets/predictors are included based on modeling needs.\nExample:\nfrom preprocess import prepare_data\nfinal_df = prepare_data(sales_combined)"
  },
  {
    "objectID": "Documentation.html#rf_fitfinal_df-area",
    "href": "Documentation.html#rf_fitfinal_df-area",
    "title": "Project Functions Documentation",
    "section": "rf_fit(final_df, area)",
    "text": "rf_fit(final_df, area)\nPurpose: Train a RandomForestRegressor to predict area using 3-fold GridSearchCV. Splits data, scales numeric features on the training set, logs target via np.log1p, evaluates on test, prints metrics and feature importances, returns best estimator.\nKey steps: - Split: train_test_split(test_size=0.2, random_state=42) - Scale: StandardScaler on ['all_time_peak', 'last_30_day_avg', 'Year', 'Rank'] - Grid: n_estimators=[50,100,200], max_depth=[5,10,15], min_samples_split=[1,3,5], min_samples_leaf=[1,2,3] (note: min_samples_split must be ≥2)\nReturns: - best_model (RandomForestRegressor) trained on the full training set via grid search.\nOutputs (printed): Best parameters, R², RMSE (log scale), top-10 feature importances.\nImplementation notes: - Replace min_samples_split values with [2,3,5] to comply with scikit-learn requirements. - Consider moving scaling into a Pipeline to ensure consistent preprocessing at inference time.\nExample:\nfrom model import rf_fit\nbest_model = rf_fit(final_df, area=\"Global_Sales\")"
  },
  {
    "objectID": "Documentation.html#predictbest_model-area-new_data",
    "href": "Documentation.html#predictbest_model-area-new_data",
    "title": "Project Functions Documentation",
    "section": "predict(best_model, area, new_data)",
    "text": "predict(best_model, area, new_data)\nPurpose: Scale numeric features in new_data, predict with best_model, and inverse the log transform via np.expm1 to obtain predictions on the original scale.\nParameters: - best_model: fitted estimator from rf_fit - area: target name (not used in current implementation) - new_data: DataFrame with the same predictor columns used during training\nReturns: - preds_exp (np.ndarray): Predictions in the original target scale.\nImplementation caveat: A new StandardScaler() is created and transform is called without fitting to training statistics, which will raise an error and/or cause inconsistency. Persist and reuse the fitted scaler, or use a Pipeline.\nExample:\nfrom model import predict\n# Ensure consistent scaling via a Pipeline or by reusing the trained scaler\npreds = predict(best_model, area=\"Global_Sales\", new_data=new_X)"
  },
  {
    "objectID": "Documentation.html#print_genre_distributionsales-genre-area",
    "href": "Documentation.html#print_genre_distributionsales-genre-area",
    "title": "Project Functions Documentation",
    "section": "print_genre_distribution(sales, genre, area)",
    "text": "print_genre_distribution(sales, genre, area)\nPurpose: Plot a histogram (Seaborn histplot) of area restricted to rows whose Genre contains the given substring. Shows the plot via plt.show().\nParameters: - sales: DataFrame of sales records - genre: Substring to match within Genre (case-sensitive) - area: Numeric column to plot (e.g., Global_Sales)\nExample:\nfrom viz import print_genre_distribution\nprint_genre_distribution(sales, genre=\"Action\", area=\"Global_Sales\")"
  },
  {
    "objectID": "Documentation.html#print_platform_distributionsales-platform-area",
    "href": "Documentation.html#print_platform_distributionsales-platform-area",
    "title": "Project Functions Documentation",
    "section": "print_platform_distribution(sales, platform, area)",
    "text": "print_platform_distribution(sales, platform, area)\nPurpose: Plot a histogram of area restricted to rows whose Platform contains the given substring. Shows the plot via plt.show().\nParameters: - sales: DataFrame of sales records - platform: Substring to match within Platform (case-sensitive) - area: Numeric column to plot (e.g., Global_Sales)\nExample:\nfrom viz import print_platform_distribution\nprint_platform_distribution(sales, platform=\"PS4\", area=\"Global_Sales\")"
  },
  {
    "objectID": "Tutorial.html",
    "href": "Tutorial.html",
    "title": "Getting Started",
    "section": "",
    "text": "This short tutorial shows common workflows: reading the bundled data, processing it, training the Random Forest model, and plotting basic distributions.\n\n\nInstall the package in editable mode (for development) or normally:\npip install -e .\n# or\npip install .\n\n\n\nCopy the following into a Python session or script.\nfrom importlib.resources import files\nfrom stat386_final import (\n    read_data,\n    process_data,\n    prepare_data,\n    rf_fit,\n    print_genre_distribution,\n    print_platform_distribution,\n    __version__,\n)\n\nprint('stat386_final version:', __version__)\n\n# Load the bundled example dataset\ndata_path = files(\"stat386_final\").joinpath(\"data/game_data.csv\")\nsales = read_data(data_path)\nprint(sales.shape)\nprint(list(sales.columns)[:10])\n\n# Process and prepare for modeling\nsales_combined = process_data(sales)\nfinal_df = prepare_data(sales_combined)\nprint('Prepared dataset shape:', final_df.shape)\n\n# Fit a Random Forest model to predict global sales\n# (This will run a small grid search and print metrics)\nmodel = rf_fit(final_df, 'Global_Sales')\n\n# Quick plots (shows matplotlib windows in notebooks or interactive sessions)\nprint_genre_distribution(sales, 'Action', 'Global_Sales')\nprint_platform_distribution(sales, 'PS4', 'Global_Sales')\n\n\n\n\nThe package exposes a predict helper. In the current release the training path performs scaling internally but does not return the fitted StandardScaler. To use predict reliably you should ensure new data is prepared and scaled the same way as prepare_data did during training. If you need a prediction pipeline (model + scaler), adapt rf_fit to return the scaler alongside the model or scale new observations with the same transformation used at training time.\n\n\n\n\n\nSee the package source for implementation details: src/stat386_final\nIf you want, I can: add a train_pipeline that returns (model, scaler), add notebook examples, or run a quick import test in this environment."
  },
  {
    "objectID": "Tutorial.html#install",
    "href": "Tutorial.html#install",
    "title": "Getting Started",
    "section": "",
    "text": "Install the package in editable mode (for development) or normally:\npip install -e .\n# or\npip install ."
  },
  {
    "objectID": "Tutorial.html#quickstart-python",
    "href": "Tutorial.html#quickstart-python",
    "title": "Getting Started",
    "section": "",
    "text": "Copy the following into a Python session or script.\nfrom importlib.resources import files\nfrom stat386_final import (\n    read_data,\n    process_data,\n    prepare_data,\n    rf_fit,\n    print_genre_distribution,\n    print_platform_distribution,\n    __version__,\n)\n\nprint('stat386_final version:', __version__)\n\n# Load the bundled example dataset\ndata_path = files(\"stat386_final\").joinpath(\"data/game_data.csv\")\nsales = read_data(data_path)\nprint(sales.shape)\nprint(list(sales.columns)[:10])\n\n# Process and prepare for modeling\nsales_combined = process_data(sales)\nfinal_df = prepare_data(sales_combined)\nprint('Prepared dataset shape:', final_df.shape)\n\n# Fit a Random Forest model to predict global sales\n# (This will run a small grid search and print metrics)\nmodel = rf_fit(final_df, 'Global_Sales')\n\n# Quick plots (shows matplotlib windows in notebooks or interactive sessions)\nprint_genre_distribution(sales, 'Action', 'Global_Sales')\nprint_platform_distribution(sales, 'PS4', 'Global_Sales')"
  },
  {
    "objectID": "Tutorial.html#notes-about-predict",
    "href": "Tutorial.html#notes-about-predict",
    "title": "Getting Started",
    "section": "",
    "text": "The package exposes a predict helper. In the current release the training path performs scaling internally but does not return the fitted StandardScaler. To use predict reliably you should ensure new data is prepared and scaled the same way as prepare_data did during training. If you need a prediction pipeline (model + scaler), adapt rf_fit to return the scaler alongside the model or scale new observations with the same transformation used at training time."
  },
  {
    "objectID": "Tutorial.html#where-to-go-next",
    "href": "Tutorial.html#where-to-go-next",
    "title": "Getting Started",
    "section": "",
    "text": "See the package source for implementation details: src/stat386_final\nIf you want, I can: add a train_pipeline that returns (model, scaler), add notebook examples, or run a quick import test in this environment."
  }
]