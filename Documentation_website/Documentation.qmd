---
title: "Project Functions Documentation"
format:
  html:
    toc: true
    toc-depth: 3
  pdf:
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  message: false
---

# Overview

This document describes the public functions in the project, covering their purpose, parameters, return values, side effects, usage examples, and implementation notes. The functions are defined across the following modules: `read.py`, `preprocess.py`, `model.py`, and `viz.py`.

# Module: `read.py`

## `get_data_path(filename: str)`
**Purpose:** Return a path-like object pointing to `data/<filename>` within the package using `importlib.resources`.

**Parameters:**
- `filename` *(str)*: File name, e.g., `"sales.csv"`.

**Returns:**
- Path-like object to `data/<filename>`. 

**Example:**
```python
from read import get_data_path
csv_path = get_data_path("vgsales.csv")
```

## `read_data(file_path)`
**Purpose:** Read CSV into a DataFrame; convert `Year` to pandas nullable integer (`Int64`); drop `"Unnamed: 0"` column; return cleaned DataFrame. 

**Parameters:**
- `file_path`: String or path-like to the CSV. 

**Returns:**
- `sales` *(pd.DataFrame)*: Cleaned dataset. 

**Side effects:** Changes `Year` dtype; drops `"Unnamed: 0"` if present. 

**Example:**
```python
from read import read_data, get_data_path
sales = read_data(get_data_path("sales.csv"))
```

# Module: `preprocess.py`

## `process_data(sales)`
**Purpose:** Deduplicate and aggregate raw sales records to the `Name` level, collecting distinct categorical lists and summarizing numeric metrics. 

**Aggregations:**
- `Platform`: list of unique non-null platforms
- `Year`: max
- `Genre`: list of unique non-null genres
- `Publisher`: list of unique non-null publishers
- `all_time_peak`: max
- `last_30_day_avg`: max
- `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Global_Sales`: sum 

**Returns:**
- `sales_combined` *(pd.DataFrame)* with one row per `Name`. 

**Example:**
```python
from preprocess import process_data
sales_combined = process_data(sales)
```

## `prepare_data(sales_combined)`
**Purpose:** Ensure list types for `Platform`/`Genre`; multi-hot encode them via `MultiLabelBinarizer`; concatenate encoded features with selected numeric predictors; drop rows with missing `Year`. 

**Returns:**
- `final_df` *(pd.DataFrame)* containing:
  - Numeric predictors: `all_time_peak`, `last_30_day_avg`, `Year`, `Rank`
  - Target(s): includes `Global_Sales`
  - Encoded features: `Platform_<category>`, `Genre_<category>` 

**Example:**
```python
from preprocess import prepare_data
final_df = prepare_data(sales_combined)
```

# Module: `model.py`

## `rf_fit(final_df, area)`
**Purpose:** Train a `RandomForestRegressor` to predict `area` using 3-fold `GridSearchCV`. Splits data, scales numeric features on the training set, logs target via `np.log1p`, evaluates on test, prints metrics and feature importances, returns best estimator. 

**Key steps:**
- Split: `train_test_split(test_size=0.2, random_state=42)`
- Scale: `StandardScaler` on `['all_time_peak', 'last_30_day_avg', 'Year', 'Rank']`
- Grid: `n_estimators=[50,100,200]`, `max_depth=[5,10,15]`, `min_samples_split=[1,3,5]`, `min_samples_leaf=[1,2,3]` (note: `min_samples_split` must be ≥2) 

**Returns:**
- `best_model` *(RandomForestRegressor)* trained on the full training set via grid search. 

**Outputs (printed):** Best parameters, R², RMSE (log scale), top-10 feature importances. 

**Example:**
```python
from model import rf_fit
best_model = rf_fit(final_df, area="Global_Sales")
```

## `predict(best_model, area, new_data)`
**Purpose:** Scale numeric features in `new_data`, predict with `best_model`, and inverse the log transform via `np.expm1` to obtain predictions on the original scale. 

**Parameters:**
- `best_model`: fitted estimator from `rf_fit`
- `area`: target name (not used in current implementation)
- `new_data`: DataFrame with the same predictor columns used during training 

**Returns:**
- `preds_exp` *(np.ndarray)*: Predictions in the original target scale. 

**Example:**
```python
from model import predict
# Ensure consistent scaling via a Pipeline or by reusing the trained scaler
preds = predict(best_model, area="Global_Sales", new_data=new_X)
```

# Module: `viz.py`

## `print_genre_distribution(sales, genre, area)`
**Purpose:** Plot a histogram (Seaborn `histplot`) of `area` restricted to rows whose `Genre` contains the given substring. Shows the plot via `plt.show()`. 

**Parameters:**
- `sales`: DataFrame of sales records
- `genre`: Substring to match within `Genre` (case-sensitive)
- `area`: Numeric column to plot (e.g., `Global_Sales`) 

**Example:**
```python
from viz import print_genre_distribution
print_genre_distribution(sales, genre="Action", area="Global_Sales")
```

## `print_platform_distribution(sales, platform, area)`
**Purpose:** Plot a histogram of `area` restricted to rows whose `Platform` contains the given substring. Shows the plot via `plt.show()`. 

**Parameters:**
- `sales`: DataFrame of sales records
- `platform`: Substring to match within `Platform` (case-sensitive)
- `area`: Numeric column to plot (e.g., `Global_Sales`) 

**Example:**
```python
from viz import print_platform_distribution
print_platform_distribution(sales, platform="PS4", area="Global_Sales")
```

